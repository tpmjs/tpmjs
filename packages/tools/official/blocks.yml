name: "tpmjs-official-tools"
root: "."

# =============================================================================
# PHILOSOPHY - Core principles that guide all tool development
# =============================================================================
philosophy:
  - "Every tool MUST be a working, production-ready implementation - no stubs, no TODOs"
  - "Tools use AI SDK v6 tool() + jsonSchema() pattern exclusively"
  - "Each tool does ONE thing exceptionally well (single-shot, one call in, one result out)"
  - "Tools return structured, typed outputs that agents can reliably parse"
  - "Error handling is explicit - throw meaningful errors, never silently fail"
  - "All async operations use proper error boundaries"
  - "Dependencies are minimal and production-stable (no alpha/beta packages unless necessary)"
  - "Tools are deterministic where possible - same input yields same output"
  - "Network I/O is async but tools are single-shot (no streaming, no multi-step orchestration inside)"

# =============================================================================
# DOMAIN - Entities, signals, and measures
# =============================================================================
domain:
  entities:
    url:
      fields: [href, domain, protocol, path, query, fragment]
      description: "A fully qualified URL with parsed components"

    webpage:
      fields: [url, title, html, text, metadata]
      description: "A fetched webpage with extracted content"

    text_content:
      fields: [raw, sentences, paragraphs, wordCount]
      description: "Processed text with structural analysis"

    claim:
      fields: [statement, confidence, needsCitation, category, suggestedEvidence]
      description: "A factual assertion that can be verified"

    timeline_event:
      fields: [date, description, confidence, source, dateType]
      description: "A dated event with provenance"

    page_brief:
      fields: [url, title, summary, keyPoints, claims]
      description: "A summarized view of a webpage"

    comparison_result:
      fields: [agreements, conflicts, uniqueToA, uniqueToB, similarity]
      description: "Side-by-side analysis of two sources"

    credibility_score:
      fields: [score, signals, warnings, recommendations, confidence]
      description: "Trust assessment of a source"

    blog_post:
      fields: [title, author, content, slug, frontmatter, wordCount, readingTime]
      description: "A structured blog post with metadata"

  signals:
    credibility:
      description: "How trustworthy is this source"
      extraction_hint: "Look for HTTPS, known domains, author info, dates, citations"

    readability:
      description: "How readable is the content"
      extraction_hint: "Check sentence length, word complexity, structure"

  measures:
    working_implementation:
      constraints:
        - "Tool must have actual working code, not stubs"
        - "All dependencies must be installed and importable"
        - "Execute function must return expected output type"

    valid_output_structure:
      constraints:
        - "Output must match declared TypeScript interface"
        - "All required fields must be present"
        - "Types must match (string, number, array, etc.)"

    proper_error_handling:
      constraints:
        - "Network errors must be caught and re-thrown with context"
        - "Input validation must happen before processing"
        - "Errors must include actionable messages"

    ai_sdk_compliance:
      constraints:
        - "Must use tool() from 'ai' package"
        - "Must use jsonSchema() for input schema"
        - "Must export the tool as default"

    readme_documentation:
      constraints:
        - "README.md must exist in tool directory"
        - "README must document all inputs and outputs"
        - "README must include usage example"

# =============================================================================
# VALIDATORS
# =============================================================================
validators:
  - schema
  - shape.ts
  - domain

# =============================================================================
# BLOCKS - Default rules that apply to all blocks
# =============================================================================
blocks:
  domain_rules:
    - id: ai_sdk_v6_pattern
      description: |
        Must follow AI SDK v6 tool pattern:
        - Import { tool, jsonSchema } from 'ai'
        - Use jsonSchema<InputType>() with proper TypeScript interface
        - Wrap in tool() with description and execute function
        - Export as default

    - id: proper_json_schema
      description: |
        Input schema must be complete:
        - All properties must have type and description
        - Required fields must be listed
        - Optional fields should be marked
        - Use appropriate JSON Schema types

    - id: structured_outputs
      description: |
        Output must be structured and typed:
        - Define TypeScript interface for output
        - Return object matching interface
        - Include all fields documented in blocks.yml

    - id: async_error_handling
      description: |
        Async operations must handle errors:
        - Wrap fetch/network calls in try-catch
        - Provide meaningful error messages
        - Don't silently fail or return empty

    - id: readme_code_alignment
      description: |
        README must match implementation:
        - Documented inputs must match code
        - Documented outputs must match code
        - Usage examples must be accurate

  # ===========================================================================
  # IMPLEMENTED TOOLS
  # ===========================================================================

  research.pageBrief:
    description: "Fetches a URL, extracts main content using Readability algorithm, and returns a structured brief with summary, key points, and claims that need citations"
    path: "page-brief"
    domain_rules:
      - id: url_fetching
        description: "Must fetch URL using fetch() API with timeout"
      - id: content_extraction
        description: "Must use @mozilla/readability for content extraction"
      - id: sentence_parsing
        description: "Must parse text into sentences using sbd library"
    inputs:
      - name: url
        type: string
        description: "The URL to fetch and analyze"
    outputs:
      - name: brief
        type: PageBrief
        description: "Structured summary with key points and claims"
        measures: [working_implementation, valid_output_structure, proper_error_handling, ai_sdk_compliance, readme_documentation]

  research.comparePages:
    description: "Compares content from two URLs, identifying agreements, conflicts, and unique points using TF-IDF similarity"
    path: "compare-pages"
    domain_rules:
      - id: dual_fetch
        description: "Must fetch both URLs and handle failures gracefully"
      - id: content_comparison
        description: "Must use TF-IDF for text comparison (natural library)"
      - id: structured_diff
        description: "Must categorize into agreements/conflicts/unique"
    inputs:
      - name: urlA
        type: string
        description: "First URL to compare"
      - name: urlB
        type: string
        description: "Second URL to compare"
    outputs:
      - name: comparison
        type: PageComparison
        description: "Structured comparison result"
        measures: [working_implementation, valid_output_structure, proper_error_handling, readme_documentation]

  research.sourceCredibility:
    description: "Analyzes a URL for credibility signals using HTTPS, domain reputation, author presence, dates, and citations"
    path: "source-credibility"
    domain_rules:
      - id: url_analysis
        description: "Must parse and analyze URL structure using tldts"
      - id: html_parsing
        description: "Must parse HTML for credibility signals using cheerio"
      - id: signal_scoring
        description: "Must calculate weighted credibility score 0-1"
    inputs:
      - name: url
        type: string
        description: "The URL to analyze"
    outputs:
      - name: credibility
        type: CredibilityResult
        description: "Credibility score with signals and recommendations"
        measures: [working_implementation, valid_output_structure, proper_error_handling, readme_documentation]

  research.claimChecklist:
    description: "Extracts checkable factual claims from text with priority levels and suggested evidence types"
    path: "claim-checklist"
    domain_rules:
      - id: sentence_detection
        description: "Must use sbd for sentence boundary detection"
      - id: claim_identification
        description: "Must identify claims using pattern matching"
      - id: priority_assignment
        description: "Must assign priority levels (high/medium/low)"
    inputs:
      - name: text
        type: string
        description: "Text to extract claims from"
    outputs:
      - name: checklist
        type: ClaimChecklist
        description: "List of claims with priorities and evidence suggestions"
        measures: [working_implementation, valid_output_structure, proper_error_handling, readme_documentation]

  research.timelineFromText:
    description: "Extracts dated events from text and returns a normalized chronological timeline"
    path: "timeline-from-text"
    domain_rules:
      - id: date_parsing
        description: "Must use chrono-node for date extraction"
      - id: event_extraction
        description: "Must extract event descriptions with context"
      - id: chronological_ordering
        description: "Must sort events chronologically and identify gaps"
    inputs:
      - name: text
        type: string
        description: "Text containing dated events"
    outputs:
      - name: timeline
        type: Timeline
        description: "Chronologically ordered events with date range"
        measures: [working_implementation, valid_output_structure, proper_error_handling, readme_documentation]

  adapter.createBlogPost:
    description: "Creates a structured blog post with frontmatter, metadata, slug, word count, and reading time"
    path: "createBlogPost"
    domain_rules:
      - id: frontmatter_generation
        description: "Must generate valid YAML frontmatter"
      - id: slug_generation
        description: "Must create URL-safe slug from title"
      - id: reading_time_calculation
        description: "Must calculate reading time based on word count"
    inputs:
      - name: title
        type: string
        description: "Blog post title"
      - name: author
        type: string
        description: "Author name"
      - name: content
        type: string
        description: "Blog post content in markdown"
      - name: tags
        type: array
        optional: true
        description: "Optional tags for the post"
      - name: excerpt
        type: string
        optional: true
        description: "Optional excerpt/summary"
      - name: format
        type: string
        optional: true
        description: "Output format: markdown or mdx"
    outputs:
      - name: blogPost
        type: BlogPost
        description: "Complete blog post with frontmatter and metadata"
        measures: [working_implementation, valid_output_structure, ai_sdk_compliance, readme_documentation]
